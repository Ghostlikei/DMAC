{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shenqingyun/Desktop/git_repository/DMAC/playground\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load complete, Data[0]: \n",
      "{'label': 0, 'raw': \"I only watched the Wanda Sykes portion of this show. I think it was interesting to watch because it was before she came out as a lesbian. She was married to a man at the time. She actually made some jokes that raised my eyebrows since she is now a lesbian. I didn't like this because it seemed hypocritical but I think Wanda Sykes is really funny. She is one of the few comedians who can make me really LOL (Laugh Out Loud). If you want to see what her comedy was like before she came out a lesbian or you are a Wanda Sykes fan, watch it. I am a huge fan of hers. I would like to see her in a live how. I am glad she is true to herself now and came out as a lesbian. I hope Wanda keeps on making me and others laugh for a long time to come.\"}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "from dmac.io.loader import Project1Loader\n",
    "\n",
    "# Define a TextCNN model\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_classes, num_filters, filter_sizes):\n",
    "        super(TextCNN, self).__init()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(1, num_filters, (fs, embedding_dim))\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "        self.fc = nn.Linear(len(filter_sizes) * num_filters, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs]\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]\n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Load GloVe word embeddings\n",
    "def load_glove_embeddings(glove_file, word_to_index):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = vector\n",
    "\n",
    "    embedding_matrix = np.zeros(len(word_to_index), len(embeddings_index['the']))\n",
    "    for word, i in word_to_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "# Define hyperparameters\n",
    "max_sequence_length = 100  # Define your sequence length\n",
    "embedding_dim = 25\n",
    "num_filters = 128\n",
    "filter_sizes = [2, 3, 4]\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# Load and preprocess the data\n",
    "data = Project1Loader().load_data(\"data/exp1_data/train_data.txt\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import downloader\n",
    "\n",
    "labels = [sample[\"label\"] for sample in data]\n",
    "sentences = [sample[\"raw\"] for sample in data]\n",
    "\n",
    "# Tokenize the text and convert to numerical values\n",
    "vocab = set(word for sentence in sentences for word in sentence.split())\n",
    "word_to_index = {word: i for i, word in enumerate(vocab)}\n",
    "sequences = [[word_to_index[word] for word in sentence.split()] for sentence in sentences]\n",
    "\n",
    "# Pad sequences to a fixed length\n",
    "padded_sequences = [sequence[:max_sequence_length] + [0] * (max_sequence_length - len(sequence)) for sequence in sequences]\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create PyTorch DataLoader\n",
    "train_dataset = TensorDataset(torch.LongTensor(X_train), torch.LongTensor(y_train))\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Load pre-trained GloVe embeddings\n",
    "embedding_matrix = downloader.load(\"glove-twitter-25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the TextCNN model\n",
    "model = TextCNN(vocab_size=len(vocab), embedding_dim=embedding_dim, num_classes=10, num_filters=num_filters, filter_sizes=filter_sizes)\n",
    "model.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "model.embedding.weight.requires_grad = False  # Freeze the embedding layer\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        inputs, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for inputs in X_test:\n",
    "        inputs = torch.LongTensor(inputs).unsqueeze(0)\n",
    "        outputs = model(inputs)\n",
    "        predicted_labels = torch.argmax(outputs, 1)\n",
    "        y_pred.append(predicted_labels.item())\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_test, y_pred, average='micro')\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
